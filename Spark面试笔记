1,Spark消费Kafka,分布式情况下,如何保证信息的顺序?
    Kafka分布式的单位是Partition。如何保证消息有序,需要分几个情况讨论。
    a,使用一个Partition去存储数据,可以保证FIFO的顺序
    b,不同的Partition之间不能保证顺序。但是绝大多数据用户都可以通过message key来定义,因为同一个key
      的message可以保证只发送到同一个Partition。比如说key是userid,table row id等等，所以同一个user
      或者同一个record的消息永远只会发送到同一个Partition上,保证了同一个user或record的顺序

2,对于Spark中的数据倾斜问题你有什么好的方案？
    简单答案:避免数据源倾斜,调整并行度,使用自定义Partitioner,使用Map侧Join代替Reduce侧Join(内存表合并）
        给倾斜Key加上随机前缀

    什么是数据倾斜 数据倾斜指的是,并行处理的数据集中,某一部分的数据显著多于其他部分,从而使得该部分的处理
        速度成为整个数据集处理的瓶颈

    数据倾斜是如何造成的 在Spark中,同一个Stage的不同Partition可以并行处理，而具有依赖关系的不同stage之间
        是串行处理的。假设某个Spark Job分为Stage 0和Stage 1两个Stage,且Stage1依赖于Stage 0,那Stage 0完全
        处理结束之前不会处理Stage 1。而Stage 0可能包含 N个 Task,这N个Task可以并行进行,如果其中N-1个Task都
        在10秒内完成,而另外一个Task却耗时1分钟,那该Stage的总时间至少为1分钟。换句话说,一个Stage所耗的时间
        是由最慢的那个Task决定的。由于同一个Stage内的所有Task执行相同的计算,在排除不同计算节点计算能力差异
        的前提下,不同的Task之间耗时的差异主要由该Task所处理的数据量决定

    具体解决方案:
        1,调整并行度分散同一个Task的不同Key,Spark在做shuffle时,默认使用HashPartitioner对数据进行分区。如果
        并行度设置的不合适,可能造成大量不相同的key对应的数据分配到了同一个Task上,造成Task处理的数据远远大于
        其它Task,从而造成数据倾斜.如果调整shuffle的并行度,使得原本分配到同一个Task的不同key发配到不同Task上
        处理,则降低原Task所需处理的数据量,从而缓解数据倾斜问题造成的短板效应

        2,自定义Partitioner:使用自定义Partitioner(默认为HashPartitioner),将原本分配到同一个Task的不同key分配
        到不同的Task

        3,将Reduce side(侧)join转变为Map side(侧)join，通过Spark的Broadcasr机制,将Reduce侧Join转换为Map侧Join,
        避免shuffle从而完全消除shuffle带来的数据倾斜

        4,为skew的Key增加随机前/后缀 为数据量特别大的Key增加堆积前/后缀,使得原来的key相同的数据变为Key不相同的数据
        ,从而使倾斜的数据集分散到不同的Task中,彻底解决数据倾斜问题。数据处理完之后去除key的前缀进行二次计算

        5,大表随机添加N种随机前缀,小表扩大N倍,如果出现数据倾斜的Key比较多.直接对存在数据倾斜的数据集全部加上随机前缀，
        然后对另外一个不存在严重数据倾斜的数据集按照大表的随机前缀进行扩充（faltMap操作）,两个数据集join完之后去除
        前缀进行二次计算

3,你所理解的Spark的shuffle过程
    Spark shuffle处于一个宽依赖,可以实现类似混洗的功能,将相同的key分发到同一个reducer上进行处理

4,Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子
    尽量避免使用reduceByKey,Join，distinct，repartition等会进进行shuffle的算子,尽量使用map类的非
    shuffle算子.这样的话,没有shuffle操作或者仅有较少shuffle操作的Spark作业,可以大大减少开销

5,spark on yarn作业执行流程,yarn-client和yarn cluster有什么区别
    1,spark支持资源动态共享,运行于yarn的框架都共享一个集中配置好的资源池
    2,可以很方便的利用yarn的资源调度性来作分类,隔离以及优先级控制负载,拥有更灵活的调度策略
    3,yarn可以自由地选择executor数量
    4,yarn是唯一支持spark安全的集群管理器,使用yarn,spark可以运行于kerberized hadoop之上,在它们
    进程之间进行安全认证

    yarn-cilent和yarn cluster的异同 1,从广义上讲,yarn-cluster适用于生产环境.而yarn-client适用于
    交互和调试,也就是希望快速看到application的输出. 2,从深层的含义讲,yarn-cluster和yarn-client
    模式的区别其实就是Application Master进程的区别,yarn-cluster模式下,driver运行在AM中,它负责向
    yarn申请资源,并监督作业的运行状况.当用户提交作业之后,就可以换掉client,作业会继续在yarn上运行
    而yarn-client模式下，Application Master仅仅向yarn请求executor，Client会和请求的container通信
    来调度他们工作,client不能离开



